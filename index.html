<!--?xml version="1.0" encoding="UTF-8"?-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252">
<title>In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation</title>
<meta name="generator" content="Nested http://nestededitor.sourceforge.net/">
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    menuSettings: {zoom: "Double-Click", zscale: "300%"},
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    MathMenu: {showRenderer: false},
    "HTML-CSS": {
        availableFonts: ["TeX"],
        preferredFont: "TeX",
        imageFont: null
    }
  });
</script>
<style type="text/css">
    body { background-color: White; font-family: Helvetica, Futura, "Trebuchet MS", sans-serif; width:900px; margin:0 auto;}
    h1 { color: black; font-family:  Helvetica, Futura, "Trebuchet MS", sans-serif; font-size: 27px; }
    h2 { color: black; font-family:  Helvetica, Futura, "Trebuchet MS", sans-serif; font-size: 22px}
    h4 { color: black; font-family:  Helvetica, Futura, "Trebuchet MS", sans-serif; font-size: 15px; font-weight:normal}
    p { color: black; font-family: Helvetica, Futura, "Trebuchet MS", sans-serif;}
</style>
</head>


<body><div id="header" class="header" align="center">
<h1>In the Eye of Transformer: <br />Global-Local Correlation for Egocentric Gaze Estimation</h1>

<table style="width:90%">
  <tbody><tr>
    <td style="text-align:center"><font size="4"><a href="https://github.com/BolinLai">Bolin Lai</a> </font></td>
    <td style="text-align:center"><font size="4"><a href="https://aptx4869lm.github.io/">Miao Liu</a> </font></td>
    <td style="text-align:center"><font size="4"><a href="https://fkryan.github.io/">Fiona Ryan</a> </font></td>
    <td style="text-align:center"><font size="4"><a href="https://rehg.org/">James Rehg</a></td>
  </tr>
  <tr>
  <td style="text-align:center">Georgia Institute of Technology</td> 
  <td style="text-align:center">Georgia Institute of Technology</td>
  <td style="text-align:center">Georgia Institute of Technology</td> 
  <td style="text-align:center">Georgia Institute of Technology</td> 
  </tr>
</tbody></table>

<p>
  <!-- <a href="??" style="font-size: 24px;">[code]</a> -->
  <a style="font-size: 24px;">[code]</a> 
  <a style="font-size: 24px;">[arxiv]</a> 
  <a style="font-size: 24px;">[Annotation]</a> 
  
</p>


</div>

<div style="text-align: center;">
  <img src="teaser.png"  style="width:800px;height:175px;">
</div>

<h4>
  Example of local correlation and global-local correlation for the task of egocentric gaze estimation (predicting where the camera-wearer is looking using egocentric video alone). The red dot represents the gaze ground truth (from a wearable eye tracker) and the image patch that contains the gaze target has red edges. Global-local correlation models the connections between the global context and each local patch, making it possible to capture, e.g., the camera wearer and social partner are pointing at the salient object. In contrast, local-local correlations may not yield an effective representation of the scene context.
</h4>

<h2>Abstract</h2>
In this paper, we present the first transformer-based model to address the challenging problem of egocentric gaze estimation. We observe that the connection between the global scene context and local visual information is vital for localizing the gaze fixation from egocentric video frames. To this end, we design the transformer encoder to embed the global context as one additional visual token and further propose a novel Global-Local Correlation (GLC) module to explicitly model the correlation of the global token and each local token. We validate our model on two egocentric video datasets -- EGTEA Gaze+ and Ego4D. Our detailed ablation studies demonstrate the benefits of our method. In addition, our approach exceeds previous state-of-the-arts by a large margin. We also provide additional visualizations to support our claim that global-local correlation serves a key representation for predicting gaze fixation from egocentric videos.
<br>

<hr class="heavy">

<div style="text-align: center;">
<h2>Demo Video</h2>
<video controls="controls" width="720">
  <source src="Demo.mp4" type="video/mp4">
</video>
</div>
<hr class="heavy">

<h2 id="bibtex">Cite</h2>

If you find this work useful in your own research, please consider citing:
<!-- <pre>
@inproceedings{liu2019forecasting,
  title={Forecasting human object interaction: Joint prediction of motor attention and actions in First Person Video},
  author={Liu, Miao and Tang, Siyu and Li, Yin and Rehg, James},
  booktitle={ECCV},
  year={2020}
}
</pre> -->

<h2 id="Contact">Contact</h2>
For questions about paper, please contact bolin dot lai at gatech dot edu
<br>
</body></html>
